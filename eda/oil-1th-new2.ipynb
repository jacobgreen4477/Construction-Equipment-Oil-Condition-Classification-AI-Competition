{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install optbinning","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:06.953400Z","iopub.execute_input":"2022-11-27T06:39:06.953811Z","iopub.status.idle":"2022-11-27T06:39:18.257609Z","shell.execute_reply.started":"2022-11-27T06:39:06.953778Z","shell.execute_reply":"2022-11-27T06:39:18.256129Z"},"trusted":true},"execution_count":306,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optbinning in /opt/conda/lib/python3.7/site-packages (0.17.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from optbinning) (3.5.3)\nRequirement already satisfied: scikit-learn>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from optbinning) (1.0.2)\nRequirement already satisfied: ropwr>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from optbinning) (0.4.0)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from optbinning) (1.7.3)\nRequirement already satisfied: ortools>=7.2 in /opt/conda/lib/python3.7/site-packages (from optbinning) (9.4.1874)\nRequirement already satisfied: numpy>=1.16.1 in /opt/conda/lib/python3.7/site-packages (from optbinning) (1.21.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from optbinning) (1.3.5)\nRequirement already satisfied: absl-py>=0.13 in /opt/conda/lib/python3.7/site-packages (from ortools>=7.2->optbinning) (0.15.0)\nRequirement already satisfied: protobuf>=3.19.4 in /opt/conda/lib/python3.7/site-packages (from ortools>=7.2->optbinning) (3.19.4)\nRequirement already satisfied: cvxpy>=1.1.14 in /opt/conda/lib/python3.7/site-packages (from ropwr>=0.4.0->optbinning) (1.2.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.0->optbinning) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.0->optbinning) (1.0.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->optbinning) (4.33.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->optbinning) (9.1.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->optbinning) (21.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->optbinning) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->optbinning) (0.11.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->optbinning) (1.4.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->optbinning) (3.0.9)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->optbinning) (2022.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.13->ortools>=7.2->optbinning) (1.15.0)\nRequirement already satisfied: ecos>=2 in /opt/conda/lib/python3.7/site-packages (from cvxpy>=1.1.14->ropwr>=0.4.0->optbinning) (2.0.10)\nRequirement already satisfied: osqp>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from cvxpy>=1.1.14->ropwr>=0.4.0->optbinning) (0.6.2.post8)\nRequirement already satisfied: scs>=1.1.6 in /opt/conda/lib/python3.7/site-packages (from cvxpy>=1.1.14->ropwr>=0.4.0->optbinning) (3.2.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->optbinning) (4.4.0)\nRequirement already satisfied: qdldl in /opt/conda/lib/python3.7/site-packages (from osqp>=0.4.1->cvxpy>=1.1.14->ropwr>=0.4.0->optbinning) (0.1.5.post2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import joblib\nimport numpy as np\nimport pandas as pd\nimport gc\nimport time\nimport os\nimport sys\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve, f1_score\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_val_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV \nfrom optbinning import OptimalBinning\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.decomposition import PCA\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.auto import tqdm\nimport random\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.260629Z","iopub.execute_input":"2022-11-27T06:39:18.261009Z","iopub.status.idle":"2022-11-27T06:39:18.273364Z","shell.execute_reply.started":"2022-11-27T06:39:18.260975Z","shell.execute_reply":"2022-11-27T06:39:18.272424Z"},"trusted":true},"execution_count":307,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 500)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.275171Z","iopub.execute_input":"2022-11-27T06:39:18.275574Z","iopub.status.idle":"2022-11-27T06:39:18.294608Z","shell.execute_reply.started":"2022-11-27T06:39:18.275540Z","shell.execute_reply":"2022-11-27T06:39:18.293346Z"},"trusted":true},"execution_count":308,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(1)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.298516Z","iopub.execute_input":"2022-11-27T06:39:18.299096Z","iopub.status.idle":"2022-11-27T06:39:18.309189Z","shell.execute_reply.started":"2022-11-27T06:39:18.299047Z","shell.execute_reply":"2022-11-27T06:39:18.307981Z"},"trusted":true},"execution_count":309,"outputs":[]},{"cell_type":"code","source":"def display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances01.png')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.311448Z","iopub.execute_input":"2022-11-27T06:39:18.311870Z","iopub.status.idle":"2022-11-27T06:39:18.322621Z","shell.execute_reply.started":"2022-11-27T06:39:18.311834Z","shell.execute_reply":"2022-11-27T06:39:18.321164Z"},"trusted":true},"execution_count":310,"outputs":[]},{"cell_type":"code","source":"# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['Y_LABEL'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_target_0 = df.loc[df['Y_LABEL'] == 0, var_name].median()\n    avg_target_1 = df.loc[df['Y_LABEL'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.loc[df['Y_LABEL'] == 0, var_name], label = 'Y_LABEL == 0')\n    sns.kdeplot(df.loc[df['Y_LABEL'] == 1, var_name], label = 'Y_LABEL == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for avg_target_0 = %0.4f' % avg_target_0)\n    print('Median value for avg_target_1 = %0.4f' % avg_target_1)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.324256Z","iopub.execute_input":"2022-11-27T06:39:18.324735Z","iopub.status.idle":"2022-11-27T06:39:18.338789Z","shell.execute_reply.started":"2022-11-27T06:39:18.324697Z","shell.execute_reply":"2022-11-27T06:39:18.337348Z"},"trusted":true},"execution_count":311,"outputs":[]},{"cell_type":"code","source":"def train_model(train,test,params,stratified,num_folds,drop_features,seed_num=1):\n    \n    seed_everything(seed_num)\n    \n    # Divide in training/validation and test data\n    train_df = train.copy()\n    test_df = test.copy()\n\n    # label encoding \n    encoder = LabelEncoder()\n    categorical_features = [i for i in train_df.select_dtypes(include=['object','category']).columns.tolist() if i not in ['ID']]\n    categorical_features = [i for i in categorical_features if i in train_df.columns.tolist()]\n    for each in categorical_features:\n        train_df[each] = encoder.fit_transform(train_df[each])\n        test_df[each] = encoder.fit_transform(test_df[each])\n\n    # set training options\n    stratified = stratified\n    num_folds = num_folds\n\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed_num)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=seed_num)\n\n    # Create arrays and dataframes to store results\n    oof_preds_lgb = np.zeros(train_df.shape[0])\n    sub_preds_lgb = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['Y_LABEL','ID','SAMPLE_TRANSFER_DAY']+drop_features]\n\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['Y_LABEL'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['Y_LABEL'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['Y_LABEL'].iloc[valid_idx]\n\n        \"\"\"        \n        learning_rate=0.02,\n        num_leaves=34,\n        colsample_bytree=0.2,\n        subsample=0.8715623,\n        max_depth=4,\n        reg_alpha=0.041545473,\n        reg_lambda=0.0735294,\n        min_split_gain=0.0222415,\n        min_child_weight=39.3259775,\n        min_child_samples = 10,\n        \"\"\"\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            \n            learning_rate = params['learning_rate'],\n            num_leaves = int(round(params['num_leaves'])),\n            colsample_bytree = params['colsample_bytree'],\n            subsample = params['subsample'],\n            max_depth = int(round(params['max_depth'])),\n            reg_alpha = params['reg_alpha'],\n            reg_lambda = params['reg_lambda'],\n            min_split_gain = params['min_split_gain'],\n            min_child_weight = params['min_child_weight'],\n            min_child_samples = int(round(params['min_child_samples'])),    \n            \n            n_jobs = -1,\n            n_estimators = 10000,            \n            random_state = seed_num,\n            silent=-1,\n            deterministic=True,\n            verbose=-1\n        )\n        \n        with warnings.catch_warnings():\n            \n            warnings.filterwarnings('ignore')\n\n            clf.fit(\n                  train_x\n                , train_y\n                , eval_set=[(train_x, train_y), (valid_x, valid_y)]\n                , eval_metric= 'auc'\n                , verbose= 200\n                , early_stopping_rounds= 500\n            )\n\n        oof_preds_lgb[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds_lgb += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds_lgb[valid_idx])))\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['Y_LABEL'], oof_preds_lgb))\n\n    # Write submission file and plot feature importance\n    test_df['Y_LABEL_lgb'] = sub_preds_lgb\n\n    # vi\n    print('-'*50)\n    # display(feature_importance_df.groupby(['feature'])['importance'].sum().sort_values(ascending=False).head(20))\n    # print('-'*50)\n    # display_importances(feature_importance_df)\n\n    # find the best thred for f1-score\n    f1_score_df = pd.DataFrame()\n    for thred in [i/1000 for i in range(0,1000,1)]:\n\n        a1 = pd.DataFrame()\n        f1 = f1_score(train_df['Y_LABEL'], np.where(oof_preds_lgb>thred,1,0), average='macro')\n        a1['f1'] = [f1]\n        a1['thred'] = [thred]\n        f1_score_df = pd.concat([f1_score_df, a1], axis=0)\n\n    thred = f1_score_df.loc[f1_score_df['f1']==f1_score_df['f1'].max(),'thred'].tolist()[0]\n    print('thred:',thred)\n    print('ncol',len(feats))\n\n    # train err\n    a1 = roc_auc_score(train_df['Y_LABEL'], oof_preds_lgb)\n    print('auc:',a1)\n    f1 = f1_score(train_df['Y_LABEL'], np.where(oof_preds_lgb>thred,1,0), average='macro')\n    print('f1:',f1)\n    a1 = train_df['Y_LABEL'].value_counts()/len(train_df)\n    print('Target ratio(real):',(a1[1]))\n\n    # test err\n    test_df['TARGET'] = np.where(test_df['Y_LABEL_lgb']>thred,1,0)\n    a1 = test_df['TARGET'].value_counts()/len(test_df)\n    print('Target ratio(pred):',(a1[1]))\n    print('Target sum:',test_df['TARGET'].sum())\n\n    # save \n    train_df['Y_LABEL_lgb'] = oof_preds_lgb\n    a1 = train_df[['ID','YEAR','COMPONENT_ARBITRARY','Y_LABEL_lgb','Y_LABEL']].copy()\n    a1.to_csv('train_pred_'+str(seed_num)+'_'+str(np.round(f1,10))+'.csv', index= False)    \n    a1 = test_df[['ID','YEAR','COMPONENT_ARBITRARY','Y_LABEL_lgb']].copy()\n    a1.to_csv('test_pred_'+str(seed_num)+'_'+str(np.round(f1,10))+'.csv', index= False)\n    \n    # submit\n    a1 = test_df[['ID', 'TARGET']].copy()\n    a1 = a1.rename(columns={'TARGET':'Y_LABEL'})\n    submission_file_name = 'sample_submission_lgb_'+str(np.round(f1,4))+'.csv'\n    a1.to_csv(submission_file_name, index= False)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.340917Z","iopub.execute_input":"2022-11-27T06:39:18.341337Z","iopub.status.idle":"2022-11-27T06:39:18.375917Z","shell.execute_reply.started":"2022-11-27T06:39:18.341301Z","shell.execute_reply":"2022-11-27T06:39:18.374638Z"},"trusted":true},"execution_count":312,"outputs":[]},{"cell_type":"code","source":"def bayes_parameter_opt_lgb(\n    train, \n    opt_params, \n    init_round=15, \n    opt_round=25, \n    n_folds=3, \n    random_seed=1, \n    n_estimators=10000, \n    output_process=False, \n    drop_features=[]\n    ):   \n    \n    seed_everything(1)\n    \n    train_df = train.copy()\n\n    # label encoding \n    encoder = LabelEncoder()\n    categorical_features = [i for i in train_df.select_dtypes(include=['object','category']).columns.tolist() if i not in ['ID']]\n    categorical_features = [i for i in categorical_features if i in train_df.columns.tolist()]\n    for each in categorical_features:\n        train_df[each] = encoder.fit_transform(train_df[each])\n        \n    # feats = [f for f in train_df.columns if f not in ['Y_LABEL','ID','SAMPLE_TRANSFER_DAY']]    \n    # X = train_df[feats].copy()\n    # y = train_df['Y_LABEL'].copy()\n    \n    # prepare data\n    # train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    \n    # parameters\n    def lgb_eval(\n          learning_rate\n        , num_leaves\n        , colsample_bytree\n        , subsample\n        , max_depth\n        , reg_alpha\n        , reg_lambda\n        , min_split_gain\n        , min_child_weight\n        , min_child_samples\n\n    ):\n        \n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n        params['subsample'] = max(min(subsample, 1), 0)\n        params['max_depth'] = int(round(max_depth))        \n        params['reg_alpha'] = reg_alpha\n        params['reg_lambda'] = reg_lambda        \n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        params['min_child_samples'] = int(round(min_child_samples))\n        \n        # cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        \n        # -----        \n        \n        stratified = True\n        \n        # Cross validation model\n        if stratified:\n            folds = StratifiedKFold(n_splits= 5, shuffle=True, random_state=1)\n        else:\n            folds = KFold(n_splits= 5, shuffle=True, random_state=1)\n\n        # Create arrays and dataframes to store results\n        oof_preds_lgb = np.zeros(train_df.shape[0])\n\n        feats = [f for f in train_df.columns if f not in ['Y_LABEL','ID','SAMPLE_TRANSFER_DAY']+drop_features]\n\n        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['Y_LABEL'])):\n            train_x, train_y = train_df[feats].iloc[train_idx], train_df['Y_LABEL'].iloc[train_idx]\n            valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['Y_LABEL'].iloc[valid_idx]\n\n            # LightGBM parameters found by Bayesian optimization\n            clf = LGBMClassifier(\n\n                **params,\n\n                n_jobs = -1,\n                n_estimators = 10000,            \n                random_state = 1,\n                silent=True,\n                deterministic=True,\n                verbose=-100\n            )\n\n            with warnings.catch_warnings():\n\n                warnings.filterwarnings('ignore')\n\n                clf.fit(\n                      train_x\n                    , train_y\n                    , eval_set=[(train_x, train_y), (valid_x, valid_y)]\n                    , eval_metric= 'auc'\n                    , verbose= False\n                    , early_stopping_rounds= 500\n                )\n\n            oof_preds_lgb[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n\n        cv_result = roc_auc_score(train_df['Y_LABEL'], oof_preds_lgb)        \n       \n        # ----\n\n        return cv_result\n\n    lgbBO = BayesianOptimization(lgb_eval, opt_params, random_state=1)\n    \n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        lgbBO.maximize(init_points=init_round, n_iter=opt_round, acq='ucb')\n    \n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n    \n    a1 = lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n    file_name = 'res_tune_'+str(len(drop_features))+'_'+str(a1[0])+'.joblib'    \n    joblib.dump(a1[1],file_name)\n    \n    return a1","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.377605Z","iopub.execute_input":"2022-11-27T06:39:18.378261Z","iopub.status.idle":"2022-11-27T06:39:18.403662Z","shell.execute_reply.started":"2022-11-27T06:39:18.378223Z","shell.execute_reply":"2022-11-27T06:39:18.402253Z"},"trusted":true},"execution_count":313,"outputs":[]},{"cell_type":"code","source":"# read data\ntrain = pd.read_csv('/kaggle/input/dacon001/open/train.csv')\ntest = pd.read_csv('/kaggle/input/dacon001/open/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/dacon001/open/sample_submission.csv')\noriginal_columns = test.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.406233Z","iopub.execute_input":"2022-11-27T06:39:18.407087Z","iopub.status.idle":"2022-11-27T06:39:18.525603Z","shell.execute_reply.started":"2022-11-27T06:39:18.407020Z","shell.execute_reply":"2022-11-27T06:39:18.524352Z"},"trusted":true},"execution_count":314,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(train['AL']>10,train['Y_LABEL'])","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.531171Z","iopub.execute_input":"2022-11-27T06:39:18.531618Z","iopub.status.idle":"2022-11-27T06:39:18.560841Z","shell.execute_reply.started":"2022-11-27T06:39:18.531574Z","shell.execute_reply":"2022-11-27T06:39:18.559635Z"},"trusted":true},"execution_count":315,"outputs":[{"execution_count":315,"output_type":"execute_result","data":{"text/plain":"Y_LABEL      0    1\nAL                 \nFalse    12490  532\nTrue       402  671","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Y_LABEL</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>AL</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>False</th>\n      <td>12490</td>\n      <td>532</td>\n    </tr>\n    <tr>\n      <th>True</th>\n      <td>402</td>\n      <td>671</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train = train.loc[train['AL']<=10,:].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.562971Z","iopub.execute_input":"2022-11-27T06:39:18.563479Z","iopub.status.idle":"2022-11-27T06:39:18.577646Z","shell.execute_reply.started":"2022-11-27T06:39:18.563431Z","shell.execute_reply":"2022-11-27T06:39:18.576113Z"},"trusted":true},"execution_count":316,"outputs":[]},{"cell_type":"code","source":"# test데이터셋에 있는 변수만 선택 \ntrain = train[test.columns.to_list()+['Y_LABEL','SAMPLE_TRANSFER_DAY']].copy()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.579208Z","iopub.execute_input":"2022-11-27T06:39:18.579732Z","iopub.status.idle":"2022-11-27T06:39:18.590554Z","shell.execute_reply.started":"2022-11-27T06:39:18.579685Z","shell.execute_reply":"2022-11-27T06:39:18.589087Z"},"trusted":true},"execution_count":317,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train.loc[train['COMPONENT_ARBITRARY'].isin(high_target),:].reset_index(drop=True)\n# test = test.loc[test['COMPONENT_ARBITRARY'].isin(high_target),:].reset_index(drop=True)\n\n# train = train.loc[train['YEAR']>=2011,:].reset_index(drop=True)\n# test = test.loc[test['YEAR']>=2011,:].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.591828Z","iopub.execute_input":"2022-11-27T06:39:18.592508Z","iopub.status.idle":"2022-11-27T06:39:18.597761Z","shell.execute_reply.started":"2022-11-27T06:39:18.592469Z","shell.execute_reply":"2022-11-27T06:39:18.596827Z"},"trusted":true},"execution_count":318,"outputs":[]},{"cell_type":"markdown","source":"## // 파생변수 생성","metadata":{}},{"cell_type":"code","source":"\"\"\"\nnew features \n\"\"\"\n\ntrain['YEAR_COMPONENT'] = train[['YEAR','COMPONENT_ARBITRARY']].apply(lambda x: '-'.join(x.astype(str)),axis=1)\ntest['YEAR_COMPONENT'] = test[['YEAR','COMPONENT_ARBITRARY']].apply(lambda x: '-'.join(x.astype(str)),axis=1)\n\nvar = [i for i in test.columns.tolist() if i not in ['ID','COMPONENT_ARBITRARY']]\ntrain['zero_cnt'] = train[var].apply(lambda x:(x==0).sum(),axis=1)\ntest['zero_cnt'] = test[var].apply(lambda x:(x==0).sum(),axis=1)\n\n\"\"\"\nerr features\n\"\"\"\n\n# train['ANONYMOUS_1_2000'] = np.where(train['ANONYMOUS_1']>2000,1,0)\n# train['ANONYMOUS_2_200'] = np.where(train['ANONYMOUS_2']>200,1,0)\n# train['CO_1.5'] = np.where(train['CO']>1.5,1,0)\n# train['CR_100'] = np.where(train['CR']>100,1,0)\n# train['FE_5000'] = np.where(train['FE']>5000,1,0)\n# train['MN_5000'] = np.where(train['MN']>55,1,0)\n# train['NI_14'] = np.where(train['NI']>14,1,0)\n\n# test['ANONYMOUS_1_2000'] = np.where(test['ANONYMOUS_1']>2000,1,0)\n# test['ANONYMOUS_2_200'] = np.where(test['ANONYMOUS_2']>200,1,0)\n# test['CO_1.5'] = np.where(test['CO']>1.5,1,0)\n# test['CR_100'] = np.where(test['CR']>100,1,0)\n# test['FE_5000'] = np.where(test['FE']>5000,1,0)\n# test['MN_5000'] = np.where(test['MN']>55,1,0)\n# test['NI_14'] = np.where(test['NI']>14,1,0)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:18.599387Z","iopub.execute_input":"2022-11-27T06:39:18.599732Z","iopub.status.idle":"2022-11-27T06:39:23.095637Z","shell.execute_reply.started":"2022-11-27T06:39:18.599702Z","shell.execute_reply":"2022-11-27T06:39:23.094557Z"},"trusted":true},"execution_count":319,"outputs":[{"execution_count":319,"output_type":"execute_result","data":{"text/plain":"'\\nerr features\\n'"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\ntransformation \n\"\"\"\n\nfor var in ['ANONYMOUS_1','PQINDEX','V40','ZN','FE']:\n    \n    train[var+'_log1'] = np.log(train[var]+1)\n    test[var+'_log1'] = np.log(test[var]+1)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:23.097090Z","iopub.execute_input":"2022-11-27T06:39:23.097474Z","iopub.status.idle":"2022-11-27T06:39:23.114156Z","shell.execute_reply.started":"2022-11-27T06:39:23.097439Z","shell.execute_reply":"2022-11-27T06:39:23.112616Z"},"trusted":true},"execution_count":320,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nnormalization\n\"\"\" \n\nfor var in ['ZN','FE','CU','V40','MO','CR','NI','ANONYMOUS_1','ANONYMOUS_2']:\n    \n    ZN_min = train[var].min()\n    ZN_max = train[var].max()\n    train[var] = (train[var] - ZN_min) / (ZN_max-ZN_min)\n    test[var] = (test[var] - ZN_min) / (ZN_max-ZN_min)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:23.115773Z","iopub.execute_input":"2022-11-27T06:39:23.116130Z","iopub.status.idle":"2022-11-27T06:39:23.141039Z","shell.execute_reply.started":"2022-11-27T06:39:23.116096Z","shell.execute_reply":"2022-11-27T06:39:23.139943Z"},"trusted":true},"execution_count":321,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfeature combination\n\"\"\"\n\nseed_everything(1)\n\nfrom itertools import chain, product \ncandidate_var = ['ZN','FE','CU','V40','MO','CR','NI','ANONYMOUS_1','ANONYMOUS_2']\npairs = list(chain(product(candidate_var, candidate_var), product(candidate_var, candidate_var))) \npairs = pd.Series([sorted([i,j]) for (i,j) in set(pairs) if i!=j]).drop_duplicates().reset_index(drop=True).tolist()\npairs = sorted(pairs)\n# pairs = pairs.sort_values()\n# random.shuffle(pairs)\n# print(pairs)\n\nfor i in range(len(pairs)):\n\n    train[pairs[i][0]+'_'+pairs[i][1]] = train[pairs[i][0]] * train[pairs[i][1]]\n    test[pairs[i][0]+'_'+pairs[i][1]] = test[pairs[i][0]] * test[pairs[i][1]] ","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:23.142612Z","iopub.execute_input":"2022-11-27T06:39:23.142930Z","iopub.status.idle":"2022-11-27T06:39:23.204509Z","shell.execute_reply.started":"2022-11-27T06:39:23.142893Z","shell.execute_reply":"2022-11-27T06:39:23.203344Z"},"trusted":true},"execution_count":322,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nwoe features \n\"\"\"\n\nfor variable in ['ANONYMOUS_1','PQINDEX','V40','ZN','FE']+['ANONYMOUS_1_V40', 'ANONYMOUS_1_CR', 'ANONYMOUS_1_ZN','ANONYMOUS_1_FE']:\n\n    optb = OptimalBinning(name=variable, dtype=\"numerical\", solver=\"cp\")\n\n    x = train[variable].values\n    y = train['Y_LABEL']\n\n    optb.fit(x, y)\n\n    binning_table = optb.binning_table\n\n    a1 = binning_table.build()\n    \n    # check \n    # print('iv:',a1['IV'].sum())\n    # display(a1)\n    # binning_table.plot(metric=\"event_rate\")\n\n    train[variable+'_woe_bin'] = pd.cut(train[variable],bins=optb.splits.tolist()).astype(str)\n    test[variable+'_woe_bin'] = pd.cut(test[variable],bins=optb.splits.tolist()).astype(str)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:23.205846Z","iopub.execute_input":"2022-11-27T06:39:23.206191Z","iopub.status.idle":"2022-11-27T06:39:24.987889Z","shell.execute_reply.started":"2022-11-27T06:39:23.206161Z","shell.execute_reply":"2022-11-27T06:39:24.986966Z"},"trusted":true},"execution_count":323,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ngroup operations\n\"\"\"\n\nopt_numeric_var = ['ANONYMOUS_1_V40', 'ANONYMOUS_1_CR', 'ANONYMOUS_1_ZN','ANONYMOUS_1_FE']\n\nfor group in ['COMPONENT_ARBITRARY','YEAR_COMPONENT','YEAR']+['FE_woe_bin']:\n\n    for numeric_var in ['ZN','FE','V40','PQINDEX','CU','ANONYMOUS_1','ANONYMOUS_2']+opt_numeric_var:\n        \n        train = train.copy()\n        test = test.copy()\n\n        a1 = train.groupby([group])[numeric_var].median().to_dict()\n\n        train[numeric_var+'_'+group+'_'+'median'] = train[group].map(a1)\n        test[numeric_var+'_'+group+'_'+'median'] = test[group].map(a1)\n\n        train[numeric_var+'_minus_'+numeric_var+'_'+group+'_'+'median'] = train[numeric_var] - train[numeric_var+'_'+group+'_'+'median']\n        test[numeric_var+'_minus_'+numeric_var+'_'+group+'_'+'median'] = test[numeric_var] - test[numeric_var+'_'+group+'_'+'median']\n        \n        train[numeric_var+'_divide_'+numeric_var+'_'+group+'_'+'median'] = np.log(train[numeric_var]+0.00001) / np.log(train[numeric_var+'_'+group+'_'+'median']+0.00001) \n        test[numeric_var+'_divide_'+numeric_var+'_'+group+'_'+'median'] = np.log(test[numeric_var]+0.00001) / np.log(test[numeric_var+'_'+group+'_'+'median']+0.00001)\n        \n        \n        a1 = train.groupby([group])[numeric_var].max().to_dict()\n\n        train[numeric_var+'_'+group+'_'+'max'] = train[group].map(a1)\n        test[numeric_var+'_'+group+'_'+'max'] = test[group].map(a1)\n\n        train[numeric_var+'_minus_'+numeric_var+'_'+group+'_'+'max'] = train[numeric_var] - train[numeric_var+'_'+group+'_'+'max']\n        test[numeric_var+'_minus_'+numeric_var+'_'+group+'_'+'max'] = test[numeric_var] - test[numeric_var+'_'+group+'_'+'max']\n        \n        train[numeric_var+'_divide_'+numeric_var+'_'+group+'_'+'max'] = np.log(train[numeric_var]+0.00001) / np.log(train[numeric_var+'_'+group+'_'+'max']+0.00001)\n        test[numeric_var+'_divide_'+numeric_var+'_'+group+'_'+'max'] = np.log(test[numeric_var]+0.00001) / np.log(test[numeric_var+'_'+group+'_'+'max']+0.00001)\n        \n        \n        a1 = train.groupby([group])[numeric_var].min().to_dict()\n\n        train[numeric_var+'_'+group+'_'+'min'] = train[group].map(a1)\n        test[numeric_var+'_'+group+'_'+'min'] = test[group].map(a1)\n\n        train[numeric_var+'_minus_'+numeric_var+'_'+group+'_'+'min'] = train[numeric_var] - train[numeric_var+'_'+group+'_'+'min']\n        test[numeric_var+'_minus_'+numeric_var+'_'+group+'_'+'min'] = test[numeric_var] - test[numeric_var+'_'+group+'_'+'min']\n        \n        train[numeric_var+'_divide_'+numeric_var+'_'+group+'_'+'min'] = np.log(train[numeric_var]+0.00001) / np.log(train[numeric_var+'_'+group+'_'+'min']+0.00001) \n        test[numeric_var+'_divide_'+numeric_var+'_'+group+'_'+'min'] = np.log(test[numeric_var]+0.00001) / np.log(test[numeric_var+'_'+group+'_'+'min']+0.00001) \n        \n        \n        a1 = train.groupby([group])[numeric_var].sum().to_dict()\n\n        train[numeric_var+'_'+group+'_'+'sum'] = train[group].map(a1)\n        test[numeric_var+'_'+group+'_'+'sum'] = test[group].map(a1)\n\n        train[numeric_var+'_minus_'+numeric_var+'_'+group+'_'+'sum'] = train[numeric_var] - train[numeric_var+'_'+group+'_'+'sum']\n        test[numeric_var+'_minus_'+numeric_var+'_'+group+'_'+'sum'] = test[numeric_var] - test[numeric_var+'_'+group+'_'+'sum']\n        \n        train[numeric_var+'_divide_'+numeric_var+'_'+group+'_'+'sum'] = np.log(train[numeric_var]+0.00001) / np.log(train[numeric_var+'_'+group+'_'+'sum']+0.00001)\n        test[numeric_var+'_divide_'+numeric_var+'_'+group+'_'+'sum'] = np.log(test[numeric_var]+0.00001) / np.log(test[numeric_var+'_'+group+'_'+'sum']+0.00001)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:24.989797Z","iopub.execute_input":"2022-11-27T06:39:24.990491Z","iopub.status.idle":"2022-11-27T06:39:28.390804Z","shell.execute_reply.started":"2022-11-27T06:39:24.990448Z","shell.execute_reply":"2022-11-27T06:39:28.389698Z"},"trusted":true},"execution_count":324,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nclustetring features\n\"\"\"\n\nnp.random.seed(1)\n\nfeatures = ['ZN','FE','V40','CU','MO']\n\n# sc = MinMaxScaler((0, 1))\n# sc_fit = sc.fit(train[features])\n# train_scaled = sc_fit.transform(train[features])\n# kmeans = KMeans(n_clusters=15,n_init=25, max_iter = 600, random_state=1).fit(train_scaled)\n# test_scaled = sc_fit.transform(test[features])\n# train[\"cluster_no\"] = kmeans.predict(train_scaled)\n# test[\"cluster_no\"] = kmeans.predict(test_scaled)\n\n# 앞에서 min-max 이미 적용 함 \nkmeans = KMeans(n_clusters=15,n_init=25, max_iter = 600, random_state=1).fit(train[features])\ntrain[\"cluster_no\"] = kmeans.predict(train[features])\ntest[\"cluster_no\"] = kmeans.predict(test[features])\n\n# check\n# print(train['cluster_no'].value_counts())\n\nfor var in ['ANONYMOUS_1','ANONYMOUS_2']+['ANONYMOUS_1_V40', 'ANONYMOUS_1_CR', 'ANONYMOUS_1_ZN','ANONYMOUS_1_FE']:\n    \n    a1 = train.groupby(['cluster_no'])[var].mean().to_dict()\n    train['cluster_no_by'+'_'+var+'_'+'mean'] = train['cluster_no'].map(a1)\n    test['cluster_no_by'+'_'+var+'_'+'mean'] = test['cluster_no'].map(a1)\n    \n    a1 = train.groupby(['cluster_no'])[var].median().to_dict()\n    train['cluster_no_by'+'_'+var+'_'+'median'] = train['cluster_no'].map(a1)\n    test['cluster_no_by'+'_'+var+'_'+'median'] = test['cluster_no'].map(a1)\n\n    a1 = train.groupby(['cluster_no'])[var].max().to_dict()\n    train['cluster_no_by'+'_'+var+'_'+'max'] = train['cluster_no'].map(a1)\n    test['cluster_no_by'+'_'+var+'_'+'max'] = test['cluster_no'].map(a1)\n\n    a1 = train.groupby(['cluster_no'])[var].min().to_dict()\n    train['cluster_no_by'+'_'+var+'_'+'min'] = train['cluster_no'].map(a1)\n    test['cluster_no_by'+'_'+var+'_'+'min'] = test['cluster_no'].map(a1)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:28.392254Z","iopub.execute_input":"2022-11-27T06:39:28.392616Z","iopub.status.idle":"2022-11-27T06:39:32.623604Z","shell.execute_reply.started":"2022-11-27T06:39:28.392582Z","shell.execute_reply":"2022-11-27T06:39:32.622730Z"},"trusted":true},"execution_count":325,"outputs":[]},{"cell_type":"code","source":"# check inf values \na1 = train.select_dtypes(include=['int','float']).apply(lambda x: x.max(),axis=0).reset_index(name='val')\na1.loc[a1['val']==np.Inf,'index'].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:32.624875Z","iopub.execute_input":"2022-11-27T06:39:32.625394Z","iopub.status.idle":"2022-11-27T06:39:32.762915Z","shell.execute_reply.started":"2022-11-27T06:39:32.625363Z","shell.execute_reply":"2022-11-27T06:39:32.761883Z"},"trusted":true},"execution_count":326,"outputs":[{"execution_count":326,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\naddtional data \n\ntrain_vote = pd.read_csv('/kaggle/input/traintestvote/train_vote.csv')\ntest_vote = pd.read_csv('/kaggle/input/traintestvote/test_vote.csv')\n\na1 = test_vote.loc[test_vote['vote']>30,:]\nadditional_train1 = test.loc[test['ID'].isin(a1['ID'].tolist()),:].reset_index(drop=True)\nadditional_train1['Y_LABEL'] = 1\nprint(len(additional_train1))\n\n# a1 = test_vote.loc[test_vote['vote']==0,:]\n# additional_train0 = test.loc[test['ID'].isin(a1['ID'].tolist()),:].reset_index(drop=True)\n# additional_train0['Y_LABEL'] = 0\n# print(len(additional_train0))\n\ntrain = pd.concat([train,additional_train1],axis=0).reset_index(drop=True)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:32.764244Z","iopub.execute_input":"2022-11-27T06:39:32.764787Z","iopub.status.idle":"2022-11-27T06:39:32.771353Z","shell.execute_reply.started":"2022-11-27T06:39:32.764755Z","shell.execute_reply":"2022-11-27T06:39:32.770382Z"},"trusted":true},"execution_count":327,"outputs":[{"execution_count":327,"output_type":"execute_result","data":{"text/plain":"\"\\naddtional data \\n\\ntrain_vote = pd.read_csv('/kaggle/input/traintestvote/train_vote.csv')\\ntest_vote = pd.read_csv('/kaggle/input/traintestvote/test_vote.csv')\\n\\na1 = test_vote.loc[test_vote['vote']>30,:]\\nadditional_train1 = test.loc[test['ID'].isin(a1['ID'].tolist()),:].reset_index(drop=True)\\nadditional_train1['Y_LABEL'] = 1\\nprint(len(additional_train1))\\n\\n# a1 = test_vote.loc[test_vote['vote']==0,:]\\n# additional_train0 = test.loc[test['ID'].isin(a1['ID'].tolist()),:].reset_index(drop=True)\\n# additional_train0['Y_LABEL'] = 0\\n# print(len(additional_train0))\\n\\ntrain = pd.concat([train,additional_train1],axis=0).reset_index(drop=True)\\n\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> 모델학습","metadata":{}},{"cell_type":"code","source":"def train_model(train,test,params,stratified,num_folds,drop_features,seed_num):\n    \n    # start log \n    print('-'*50)\n    print('>> seed_num:',seed_num)   \n    print('>> drop_features:',len(drop_features))\n    \n    seed_everything(1)\n    \n    # Divide in training/validation and test data\n    train_df = train.copy()\n    test_df = test.copy()\n\n    # label encoding \n    encoder = LabelEncoder()\n    categorical_features = [i for i in train_df.select_dtypes(include=['object','category']).columns.tolist() if i not in ['ID']]\n    categorical_features = [i for i in categorical_features if i in train_df.columns.tolist()]\n    for each in categorical_features:\n        train_df[each] = encoder.fit_transform(train_df[each])\n        test_df[each] = encoder.fit_transform(test_df[each])\n\n    # set training options\n    stratified = stratified\n    num_folds = num_folds\n\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1)\n\n    # Create arrays and dataframes to store results\n    oof_preds_lgb = np.zeros(train_df.shape[0])\n    sub_preds_lgb = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['Y_LABEL','ID','SAMPLE_TRANSFER_DAY']+drop_features]\n\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['Y_LABEL'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['Y_LABEL'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['Y_LABEL'].iloc[valid_idx]\n\n        \"\"\"        \n        learning_rate=0.02,\n        num_leaves=34,\n        colsample_bytree=0.2,\n        subsample=0.8715623,\n        max_depth=4,\n        reg_alpha=0.041545473,\n        reg_lambda=0.0735294,\n        min_split_gain=0.0222415,\n        min_child_weight=39.3259775,\n        min_child_samples = 10,\n        \"\"\"\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            \n            learning_rate = params['learning_rate'],\n            num_leaves = int(round(params['num_leaves'])),\n            colsample_bytree = params['colsample_bytree'],\n            subsample = params['subsample'],\n            max_depth = int(round(params['max_depth'])),\n            reg_alpha = params['reg_alpha'],\n            reg_lambda = params['reg_lambda'],\n            min_split_gain = params['min_split_gain'],\n            min_child_weight = params['min_child_weight'],\n            min_child_samples = int(round(params['min_child_samples'])),    \n            \n            n_jobs = -1,\n            n_estimators = 10000,            \n            random_state = seed_num,\n            silent=-1,\n            deterministic=True,\n            verbose=-1\n        )\n        \n        with warnings.catch_warnings():\n            \n            warnings.filterwarnings('ignore')\n\n            clf.fit(\n                  train_x\n                , train_y\n                , eval_set=[(train_x, train_y), (valid_x, valid_y)]\n                , eval_metric= 'auc'\n                , verbose= -1\n                , early_stopping_rounds= 500\n            )\n\n        oof_preds_lgb[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds_lgb += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds_lgb[valid_idx])))\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['Y_LABEL'], oof_preds_lgb))\n\n    # Write submission file and plot feature importance\n    test_df['Y_LABEL_lgb'] = sub_preds_lgb\n\n    # vi\n    # print('-'*50)\n    display(feature_importance_df.groupby(['feature'])['importance'].sum().sort_values(ascending=False).head(30))\n    # print('-'*50)\n    # display_importances(feature_importance_df)\n    \n    # train auc\n    oof_auc = roc_auc_score(train_df['Y_LABEL'], oof_preds_lgb)\n\n    \n    if oof_auc>=0.701:\n\n        # find the best thred for f1-score\n        f1_score_df = pd.DataFrame()\n        for thred in [i/10000 for i in range(0,10000,1) if (i/10000>0.1) & (i/10000<0.35)]:\n\n            a1 = pd.DataFrame()\n            f1 = f1_score(train_df['Y_LABEL'], np.where(oof_preds_lgb>thred,1,0), average='macro')\n            a1['f1'] = [f1]\n            a1['thred'] = [thred]\n            f1_score_df = pd.concat([f1_score_df, a1], axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:32.772892Z","iopub.execute_input":"2022-11-27T06:39:32.773340Z","iopub.status.idle":"2022-11-27T06:39:32.809434Z","shell.execute_reply.started":"2022-11-27T06:39:32.773270Z","shell.execute_reply":"2022-11-27T06:39:32.808104Z"},"trusted":true},"execution_count":328,"outputs":[]},{"cell_type":"code","source":"def train_model(train,test,params,stratified,num_folds,drop_features,seed_num):\n    \n    # start log \n    print('-'*50)\n    print('>> seed_num:',seed_num)   \n    print('>> drop_features:',len(drop_features))\n    \n    seed_everything(1)\n    \n    # Divide in training/validation and test data\n    train_df = train.copy()\n    test_df = test.copy()\n\n    # label encoding \n    encoder = LabelEncoder()\n    categorical_features = [i for i in train_df.select_dtypes(include=['object','category']).columns.tolist() if i not in ['ID']]\n    categorical_features = [i for i in categorical_features if i in train_df.columns.tolist()]\n    for each in categorical_features:\n        train_df[each] = encoder.fit_transform(train_df[each])\n        test_df[each] = encoder.fit_transform(test_df[each])\n\n    # set training options\n    stratified = stratified\n    num_folds = num_folds\n\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1)\n\n    # Create arrays and dataframes to store results\n    oof_preds_lgb = np.zeros(train_df.shape[0])\n    sub_preds_lgb = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['Y_LABEL','ID','SAMPLE_TRANSFER_DAY']+drop_features]\n    \n    for seed in seed_num:\n        \n        print(seed)\n        \n        oof_preds_lgb2 = np.zeros(train_df.shape[0])\n        sub_preds_lgb2 = np.zeros(test_df.shape[0])\n\n        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['Y_LABEL'])):\n            train_x, train_y = train_df[feats].iloc[train_idx], train_df['Y_LABEL'].iloc[train_idx]\n            valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['Y_LABEL'].iloc[valid_idx]\n\n            \"\"\"        \n            learning_rate=0.02,\n            num_leaves=34,\n            colsample_bytree=0.2,\n            subsample=0.8715623,\n            max_depth=4,\n            reg_alpha=0.041545473,\n            reg_lambda=0.0735294,\n            min_split_gain=0.0222415,\n            min_child_weight=39.3259775,\n            min_child_samples = 10,\n            \"\"\"\n\n            # LightGBM parameters found by Bayesian optimization\n            clf = LGBMClassifier(\n\n                learning_rate = params['learning_rate'],\n                num_leaves = int(round(params['num_leaves'])),\n                colsample_bytree = params['colsample_bytree'],\n                subsample = params['subsample'],\n                max_depth = int(round(params['max_depth'])),\n                reg_alpha = params['reg_alpha'],\n                reg_lambda = params['reg_lambda'],\n                min_split_gain = params['min_split_gain'],\n                min_child_weight = params['min_child_weight'],\n                min_child_samples = int(round(params['min_child_samples'])),    \n\n                n_jobs = -1,\n                n_estimators = 10000,            \n                random_state = seed,\n                silent=-1,\n                deterministic=True,\n                verbose=-1\n            )\n\n            with warnings.catch_warnings():\n\n                warnings.filterwarnings('ignore')\n\n                clf.fit(\n                      train_x\n                    , train_y\n                    , eval_set=[(train_x, train_y), (valid_x, valid_y)]\n                    , eval_metric= 'auc'\n                    , verbose= -1\n                    , early_stopping_rounds= 500\n                )\n\n            oof_preds_lgb2[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n            sub_preds_lgb2 += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n\n\n            print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds_lgb2[valid_idx])))\n            \n        oof_preds_lgb += oof_preds_lgb2/len(seed_num)\n        sub_preds_lgb += sub_preds_lgb2/len(seed_num)\n        \n        \n        \n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['Y_LABEL'], oof_preds_lgb))\n\n    # Write submission file and plot feature importance\n    test_df['Y_LABEL_lgb'] = sub_preds_lgb\n\n    # vi\n    # print('-'*50)\n    # display(feature_importance_df.groupby(['feature'])['importance'].sum().sort_values(ascending=False).head(30))\n    # print('-'*50)\n    # display_importances(feature_importance_df)\n    \n    # train auc\n    oof_auc = roc_auc_score(train_df['Y_LABEL'], oof_preds_lgb)\n\n    \n    if oof_auc>=0.701:\n\n        # find the best thred for f1-score\n        f1_score_df = pd.DataFrame()\n        for thred in [i/10000 for i in range(0,10000,1) if (i/10000>0.1) & (i/10000<0.35)]:\n\n            a1 = pd.DataFrame()\n            f1 = f1_score(train_df['Y_LABEL'], np.where(oof_preds_lgb>thred,1,0), average='macro')\n            a1['f1'] = [f1]\n            a1['thred'] = [thred]\n            f1_score_df = pd.concat([f1_score_df, a1], axis=0)\n\n        thred = f1_score_df.loc[f1_score_df['f1']==f1_score_df['f1'].max(),'thred'].tolist()[0]\n        print('thred:',thred)\n        print('ncol',len(feats))\n\n        # train f1\n        print('auc:',oof_auc)\n        oof_f1 = f1_score(train_df['Y_LABEL'], np.where(oof_preds_lgb>thred,1,0), average='macro')\n        print('f1:',oof_f1)\n        a1 = train_df['Y_LABEL'].value_counts()/len(train_df)\n        print('Target ratio(real):',(a1[1]))\n\n        # test err\n        test_df['TARGET'] = np.where(test_df['Y_LABEL_lgb']>thred,1,0)\n        a1 = test_df['TARGET'].value_counts()/len(test_df)\n        print('Target ratio(pred):',(a1[1]))\n        target_sum = test_df['TARGET'].sum()\n        print('Target sum:',target_sum)        \n        \n        # save \n        train_df['Y_LABEL_lgb'] = oof_preds_lgb\n        a1 = train_df[['ID','YEAR','COMPONENT_ARBITRARY','Y_LABEL_lgb','Y_LABEL']].copy()\n        a1.to_csv('train_pred_'+str(seed_num)+'_'+str(np.round(oof_f1,10))+'.csv', index= False)    \n        a1 = test_df[['ID','YEAR','COMPONENT_ARBITRARY','Y_LABEL_lgb']].copy()\n        a1.to_csv('test_pred_'+str(seed_num)+'_'+str(np.round(oof_f1,10))+'.csv', index= False)\n\n        # submit\n        a1 = test_df[['ID', 'TARGET']].copy()\n        a1 = a1.rename(columns={'TARGET':'Y_LABEL'})\n        submission_file_name = 'sample_submission_lgb_'+str(np.round(oof_f1,4))+'.csv'\n        a1.to_csv(submission_file_name, index= False)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:32.812699Z","iopub.execute_input":"2022-11-27T06:39:32.813377Z","iopub.status.idle":"2022-11-27T06:39:32.848294Z","shell.execute_reply.started":"2022-11-27T06:39:32.813313Z","shell.execute_reply":"2022-11-27T06:39:32.847024Z"},"trusted":true},"execution_count":329,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nbaseline model \n\"\"\"\n\ndrop_features_vc1 = ['ZN_minus_ZN_YEAR_COMPONENT_median', 'ANONYMOUS_1_CU', 'ANONYMOUS_1_minus_ANONYMOUS_1_YEAR_COMPONENT_min', 'PQINDEX_divide_PQINDEX_YEAR_min', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_FE_woe_bin_max', 'ANONYMOUS_1_divide_ANONYMOUS_1_YEAR_COMPONENT_min', 'V40_minus_V40_FE_woe_bin_sum', 'PQINDEX_minus_PQINDEX_FE_woe_bin_sum', 'V40_minus_V40_YEAR_COMPONENT_min', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_median', 'PQINDEX_divide_PQINDEX_YEAR_COMPONENT_sum', 'FE_divide_FE_YEAR_max', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_COMPONENT_ARBITRARY_median', 'PQINDEX_minus_PQINDEX_YEAR_COMPONENT_max', 'PQINDEX_minus_PQINDEX_YEAR_COMPONENT_median', 'FE_minus_FE_YEAR_COMPONENT_median', 'ZN_divide_ZN_YEAR_min', 'PQINDEX_divide_PQINDEX_YEAR_median', 'FE_minus_FE_YEAR_COMPONENT_max', 'ZN_divide_ZN_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_FE_woe_bin_median', 'ANONYMOUS_1_ZN_YEAR_COMPONENT_max', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_YEAR_COMPONENT_max', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_COMPONENT_sum', 'ANONYMOUS_1_FE_YEAR_median', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_FE_woe_bin_sum', 'CU_divide_CU_COMPONENT_ARBITRARY_sum', 'FE_divide_FE_YEAR_median', 'ANONYMOUS_2_YEAR_max', 'FE_divide_FE_FE_woe_bin_min', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_FE_woe_bin_sum', 'V40_minus_V40_YEAR_COMPONENT_max', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_YEAR_sum', 'V40_divide_V40_FE_woe_bin_min', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_YEAR_COMPONENT_median', 'ZN_minus_ZN_YEAR_COMPONENT_min', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_YEAR_COMPONENT_max', 'V40_YEAR_median', 'V40_minus_V40_YEAR_sum', 'ANONYMOUS_1_divide_ANONYMOUS_1_YEAR_COMPONENT_max', 'NI_V40', 'V40_minus_V40_COMPONENT_ARBITRARY_min', 'PQINDEX_divide_PQINDEX_YEAR_COMPONENT_max', 'ZN_minus_ZN_YEAR_min', 'FE_divide_FE_YEAR_COMPONENT_max', 'ZN_divide_ZN_FE_woe_bin_min', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_FE_woe_bin_median', 'CU_minus_CU_YEAR_COMPONENT_median', 'ANONYMOUS_1_minus_ANONYMOUS_1_YEAR_max', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_FE_woe_bin_sum', 'ZN_divide_ZN_YEAR_max', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_FE_woe_bin_max', 'ANONYMOUS_2_divide_ANONYMOUS_2_FE_woe_bin_max', 'ZN_minus_ZN_FE_woe_bin_median', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_COMPONENT_ARBITRARY_max', 'FE_minus_FE_YEAR_max', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_COMPONENT_ARBITRARY_min', 'CU_YEAR_COMPONENT_median', 'cluster_no_by_ANONYMOUS_1_FE_median', 'ZN_YEAR_COMPONENT_sum', 'CU_minus_CU_FE_woe_bin_max', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_YEAR_COMPONENT_median', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_YEAR_median', 'CU_divide_CU_YEAR_COMPONENT_max', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_FE_woe_bin_max', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_YEAR_COMPONENT_min', 'CU_divide_CU_YEAR_max', 'ZN_divide_ZN_COMPONENT_ARBITRARY_max', 'FE_divide_FE_FE_woe_bin_max', 'cluster_no_by_ANONYMOUS_1_median', 'V40_divide_V40_FE_woe_bin_sum', 'V40_divide_V40_COMPONENT_ARBITRARY_max', 'ANONYMOUS_2_divide_ANONYMOUS_2_YEAR_COMPONENT_sum', 'FE_YEAR_COMPONENT_median', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_COMPONENT_ARBITRARY_median', 'FE_divide_FE_COMPONENT_ARBITRARY_median', 'V40_divide_V40_YEAR_COMPONENT_max', 'V40_minus_V40_YEAR_min', 'ANONYMOUS_2_divide_ANONYMOUS_2_YEAR_median', 'FE_minus_FE_FE_woe_bin_median', 'FE_minus_FE_YEAR_median', 'ANONYMOUS_1_FE_YEAR_COMPONENT_sum', 'ZN_minus_ZN_FE_woe_bin_sum', 'CU_minus_CU_FE_woe_bin_sum', 'V40', 'ANONYMOUS_1_ZN_woe_bin', 'FE_divide_FE_COMPONENT_ARBITRARY_min', 'CU_divide_CU_FE_woe_bin_sum', 'cluster_no_by_ANONYMOUS_2_mean', 'CU', 'V40_minus_V40_COMPONENT_ARBITRARY_max', 'ANONYMOUS_1_minus_ANONYMOUS_1_FE_woe_bin_median', 'V40_divide_V40_YEAR_min', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_COMPONENT_ARBITRARY_sum', 'ZN_minus_ZN_YEAR_COMPONENT_max', 'ANONYMOUS_2_minus_ANONYMOUS_2_YEAR_COMPONENT_median', 'cluster_no_by_ANONYMOUS_1_V40_mean', 'FE_minus_FE_FE_woe_bin_min', 'PQINDEX_divide_PQINDEX_YEAR_max', 'ANONYMOUS_2_minus_ANONYMOUS_2_FE_woe_bin_sum', 'FE_divide_FE_YEAR_sum', 'V40_minus_V40_YEAR_COMPONENT_median', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_FE_woe_bin_max', 'PQINDEX_divide_PQINDEX_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_divide_ANONYMOUS_1_YEAR_sum', 'ANONYMOUS_1_V40', 'ZN_divide_ZN_YEAR_sum', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_YEAR_COMPONENT_max', 'ANONYMOUS_1_divide_ANONYMOUS_1_FE_woe_bin_median', 'ANONYMOUS_2_minus_ANONYMOUS_2_COMPONENT_ARBITRARY_sum', 'PQINDEX_minus_PQINDEX_FE_woe_bin_median', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_YEAR_max', 'PQINDEX_divide_PQINDEX_FE_woe_bin_median', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_YEAR_sum', 'ZN_divide_ZN_YEAR_median', 'ZN_minus_ZN_YEAR_COMPONENT_sum', 'ANONYMOUS_2_divide_ANONYMOUS_2_YEAR_COMPONENT_median', 'V40_minus_V40_YEAR_max', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_FE_woe_bin_min', 'CR_FE', 'FE_V40', 'FE_divide_FE_YEAR_COMPONENT_min', 'V40_divide_V40_YEAR_COMPONENT_median', 'ANONYMOUS_1_FE', 'FE_divide_FE_COMPONENT_ARBITRARY_max', 'PQINDEX_minus_PQINDEX_YEAR_median', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_COMPONENT_ARBITRARY_median', 'ZN_divide_ZN_YEAR_COMPONENT_median', 'ANONYMOUS_1_minus_ANONYMOUS_1_COMPONENT_ARBITRARY_max', 'CU_divide_CU_YEAR_median', 'ZN_divide_ZN_YEAR_COMPONENT_min', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_FE_woe_bin_max', 'PQINDEX_divide_PQINDEX_FE_woe_bin_max', 'ANONYMOUS_1_divide_ANONYMOUS_1_COMPONENT_ARBITRARY_sum', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_COMPONENT_ARBITRARY_max', 'ANONYMOUS_1_minus_ANONYMOUS_1_YEAR_COMPONENT_max', 'PQINDEX_divide_PQINDEX_YEAR_COMPONENT_median', 'ANONYMOUS_1_MO', 'CU_FE', 'ZN_minus_ZN_YEAR_sum', 'ZN_minus_ZN_COMPONENT_ARBITRARY_max', 'V40_divide_V40_FE_woe_bin_max', 'PQINDEX_divide_PQINDEX_COMPONENT_ARBITRARY_max', 'ANONYMOUS_1_divide_ANONYMOUS_1_FE_woe_bin_sum', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_YEAR_median', 'PQINDEX_divide_PQINDEX_YEAR_COMPONENT_min', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_YEAR_min', 'PQINDEX_divide_PQINDEX_FE_woe_bin_min', 'PQINDEX_divide_PQINDEX_COMPONENT_ARBITRARY_min', 'ANONYMOUS_2_divide_ANONYMOUS_2_COMPONENT_ARBITRARY_median', 'PQINDEX_YEAR_COMPONENT_sum', 'PQINDEX_divide_PQINDEX_COMPONENT_ARBITRARY_sum', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_FE_woe_bin_median', 'PQINDEX_minus_PQINDEX_YEAR_COMPONENT_sum', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_COMPONENT_ARBITRARY_max', 'CU_ZN', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_YEAR_COMPONENT_max', 'ZN_divide_ZN_YEAR_COMPONENT_max', 'V40_minus_V40_FE_woe_bin_min', 'NI_ZN', 'V40_divide_V40_FE_woe_bin_median', 'ZN_minus_ZN_YEAR_max', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_YEAR_median', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_YEAR_max', 'ANONYMOUS_1_ZN', 'PQINDEX_YEAR_median', 'NI', 'FE_ZN', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_FE_woe_bin_max', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_FE_woe_bin_sum', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_YEAR_median', 'CR_V40', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_YEAR_sum', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_YEAR_COMPONENT_median', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_min', 'ANONYMOUS_2_divide_ANONYMOUS_2_COMPONENT_ARBITRARY_max', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_COMPONENT_ARBITRARY_min', 'MO_V40', 'PQINDEX_minus_PQINDEX_FE_woe_bin_max', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_COMPONENT_ARBITRARY_min', 'ANONYMOUS_1_minus_ANONYMOUS_1_YEAR_median', 'FE_minus_FE_YEAR_COMPONENT_sum', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_YEAR_max', 'cluster_no_by_ANONYMOUS_1_V40_max', 'ANONYMOUS_1_divide_ANONYMOUS_1_YEAR_max', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_YEAR_COMPONENT_min', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_COMPONENT_ARBITRARY_min', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_FE_woe_bin_median', 'ANONYMOUS_2_MO', 'PQINDEX_log1', 'CU_minus_CU_YEAR_COMPONENT_max', 'FE_NI', 'ANONYMOUS_1_divide_ANONYMOUS_1_FE_woe_bin_max', 'CU_divide_CU_FE_woe_bin_median']\ndrop_features_vc2 = ['PQINDEX_divide_PQINDEX_COMPONENT_ARBITRARY_median', 'ZN_minus_ZN_YEAR_COMPONENT_min', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_YEAR_sum', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_median', 'PQINDEX_divide_PQINDEX_YEAR_median', 'ANONYMOUS_2_minus_ANONYMOUS_2_YEAR_COMPONENT_max', 'FE_divide_FE_FE_woe_bin_min', 'V40_divide_V40_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_divide_ANONYMOUS_1_YEAR_COMPONENT_max', 'ANONYMOUS_1_divide_ANONYMOUS_1_COMPONENT_ARBITRARY_sum', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_FE_woe_bin_sum', 'FE_divide_FE_YEAR_COMPONENT_sum', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_COMPONENT_ARBITRARY_median', 'FE_divide_FE_COMPONENT_ARBITRARY_max', 'CU_divide_CU_YEAR_max', 'FE_divide_FE_YEAR_sum', 'V40_divide_V40_YEAR_min', 'FE_divide_FE_FE_woe_bin_max', 'PQINDEX_divide_PQINDEX_YEAR_COMPONENT_median', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_YEAR_COMPONENT_sum', 'ANONYMOUS_2_divide_ANONYMOUS_2_FE_woe_bin_max', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_COMPONENT_sum', 'V40_divide_V40_FE_woe_bin_sum', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_YEAR_COMPONENT_median', 'ANONYMOUS_1_FE_YEAR_median', 'ZN_divide_ZN_YEAR_min', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_min', 'ZN_minus_ZN_FE_woe_bin_sum', 'PQINDEX_divide_PQINDEX_YEAR_min', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_FE_woe_bin_min', 'CU_divide_CU_YEAR_COMPONENT_min', 'ANONYMOUS_2_YEAR_max', 'CU_minus_CU_FE_woe_bin_sum', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_YEAR_max', 'FE_minus_FE_FE_woe_bin_sum', 'FE_divide_FE_YEAR_max', 'PQINDEX_minus_PQINDEX_FE_woe_bin_max', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_sum', 'FE_minus_FE_YEAR_max', 'ANONYMOUS_1_minus_ANONYMOUS_1_YEAR_COMPONENT_min', 'FE_minus_FE_FE_woe_bin_min', 'cluster_no_by_ANONYMOUS_1_FE_median', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_YEAR_COMPONENT_median', 'ZN_minus_ZN_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_FE_woe_bin_sum', 'ZN_minus_ZN_YEAR_sum', 'NI_V40', 'ANONYMOUS_2_minus_ANONYMOUS_2_YEAR_max', 'CU_minus_CU_YEAR_sum', 'V40_minus_V40_FE_woe_bin_min', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_FE_woe_bin_median', 'PQINDEX_divide_PQINDEX_FE_woe_bin_max', 'ZN_minus_ZN_FE_woe_bin_max', 'V40_minus_V40_COMPONENT_ARBITRARY_min', 'FE_minus_FE_YEAR_COMPONENT_median', 'ANONYMOUS_1_FE', 'PQINDEX_minus_PQINDEX_FE_woe_bin_median', 'PQINDEX_minus_PQINDEX_YEAR_max', 'FE_YEAR_COMPONENT_median', 'V40_divide_V40_COMPONENT_ARBITRARY_min', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_FE_woe_bin_sum', 'ZN_minus_ZN_COMPONENT_ARBITRARY_max', 'ANONYMOUS_1_CR', 'CU_minus_CU_YEAR_COMPONENT_max', 'CU_minus_CU_FE_woe_bin_max', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_YEAR_COMPONENT_max', 'FE', 'V40_minus_V40_YEAR_min', 'ZN_divide_ZN_YEAR_COMPONENT_max', 'V40_divide_V40_YEAR_COMPONENT_median', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_FE_woe_bin_min', 'V40_divide_V40_COMPONENT_ARBITRARY_max', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_max', 'CU_divide_CU_YEAR_COMPONENT_max', 'NI_ZN', 'ANONYMOUS_2_divide_ANONYMOUS_2_YEAR_COMPONENT_sum', 'FE_YEAR_COMPONENT_max', 'FE_divide_FE_YEAR_median', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_FE_woe_bin_median', 'ZN_divide_ZN_YEAR_COMPONENT_min', 'CU_divide_CU_YEAR_COMPONENT_sum', 'FE_minus_FE_FE_woe_bin_median', 'ANONYMOUS_2_divide_ANONYMOUS_2_FE_woe_bin_sum', 'CU_divide_CU_FE_woe_bin_sum', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_COMPONENT_ARBITRARY_median', 'FE_divide_FE_YEAR_COMPONENT_min', 'V40_minus_V40_FE_woe_bin_sum', 'ANONYMOUS_2_divide_ANONYMOUS_2_COMPONENT_ARBITRARY_max', 'V40_divide_V40_COMPONENT_ARBITRARY_sum', 'ANONYMOUS_1_divide_ANONYMOUS_1_YEAR_COMPONENT_median', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_YEAR_median', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_FE_woe_bin_sum', 'PQINDEX_minus_PQINDEX_FE_woe_bin_sum', 'ANONYMOUS_1_minus_ANONYMOUS_1_YEAR_COMPONENT_max', 'ANONYMOUS_2_minus_ANONYMOUS_2_YEAR_sum', 'CR_ZN', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_YEAR_max', 'CR_V40', 'V40_minus_V40_FE_woe_bin_max', 'V40_minus_V40_YEAR_COMPONENT_sum', 'ZN_divide_ZN_COMPONENT_ARBITRARY_max', 'CU_divide_CU_COMPONENT_ARBITRARY_sum', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_FE_woe_bin_max', 'ANONYMOUS_1_divide_ANONYMOUS_1_YEAR_COMPONENT_min', 'ZN_minus_ZN_YEAR_COMPONENT_max', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_YEAR_median', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_YEAR_COMPONENT_sum', 'PQINDEX_divide_PQINDEX_YEAR_COMPONENT_sum', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_YEAR_min', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_YEAR_median', 'FE_minus_FE_YEAR_median', 'ANONYMOUS_1_minus_ANONYMOUS_1_COMPONENT_ARBITRARY_min', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_YEAR_max', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_COMPONENT_ARBITRARY_max', 'V40_divide_V40_YEAR_max', 'ANONYMOUS_2_MO', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_YEAR_COMPONENT_sum', 'V40_divide_V40_YEAR_COMPONENT_sum', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_COMPONENT_ARBITRARY_min', 'PQINDEX_divide_PQINDEX_FE_woe_bin_sum', 'ANONYMOUS_2_divide_ANONYMOUS_2_YEAR_COMPONENT_median', 'MO_NI', 'MO_ZN', 'ANONYMOUS_1_minus_ANONYMOUS_1_FE_woe_bin_median', 'ANONYMOUS_2_minus_ANONYMOUS_2_YEAR_COMPONENT_median', 'ZN_minus_ZN_YEAR_COMPONENT_median', 'CR_MO', 'CR_FE', 'ZN_divide_ZN_FE_woe_bin_max', 'V40_minus_V40_YEAR_COMPONENT_max', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_YEAR_median', 'FE_divide_FE_YEAR_min', 'PQINDEX_minus_PQINDEX_YEAR_COMPONENT_median', 'V40_woe_bin', 'PQINDEX_divide_PQINDEX_FE_woe_bin_min', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_YEAR_median', 'CU_YEAR_COMPONENT_max', 'ANONYMOUS_2_YEAR_COMPONENT_max', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_FE_woe_bin_min', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_FE_woe_bin_max', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_FE_woe_bin_median', 'ZN_minus_ZN_FE_woe_bin_median', 'ANONYMOUS_2_minus_ANONYMOUS_2_YEAR_COMPONENT_sum', 'PQINDEX_divide_PQINDEX_YEAR_COMPONENT_min', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_YEAR_sum', 'FE_NI', 'ANONYMOUS_1_divide_ANONYMOUS_1_FE_woe_bin_sum', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_YEAR_sum', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_YEAR_COMPONENT_sum', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_YEAR_sum', 'NI', 'ZN_minus_ZN_YEAR_COMPONENT_sum', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_FE_woe_bin_max', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_FE_woe_bin_max', 'ZN_divide_ZN_YEAR_COMPONENT_median', 'V40_YEAR_median', 'PQINDEX_divide_PQINDEX_COMPONENT_ARBITRARY_sum', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_COMPONENT_ARBITRARY_min', 'V40_divide_V40_FE_woe_bin_median', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_YEAR_COMPONENT_max', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_FE_woe_bin_min', 'ANONYMOUS_1_CU', 'CU_divide_CU_FE_woe_bin_max', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_COMPONENT_ARBITRARY_sum', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_FE_woe_bin_max', 'V40', 'ANONYMOUS_1_divide_ANONYMOUS_1_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_log1', 'ANONYMOUS_2_minus_ANONYMOUS_2_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_YEAR_max', 'V40_divide_V40_FE_woe_bin_min', 'ANONYMOUS_1_FE_divide_ANONYMOUS_1_FE_FE_woe_bin_sum']\ndrop_features_vc3 = ['PQINDEX_divide_PQINDEX_COMPONENT_ARBITRARY_median', 'ZN_minus_ZN_YEAR_COMPONENT_min', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_YEAR_sum', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_median', 'PQINDEX_divide_PQINDEX_YEAR_median', 'ANONYMOUS_2_minus_ANONYMOUS_2_YEAR_COMPONENT_max', 'FE_divide_FE_FE_woe_bin_min', 'V40_divide_V40_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_divide_ANONYMOUS_1_YEAR_COMPONENT_max', 'ANONYMOUS_1_divide_ANONYMOUS_1_COMPONENT_ARBITRARY_sum', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_FE_woe_bin_sum', 'FE_divide_FE_YEAR_COMPONENT_sum', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_COMPONENT_ARBITRARY_median', 'FE_divide_FE_COMPONENT_ARBITRARY_max', 'CU_divide_CU_YEAR_max', 'FE_divide_FE_YEAR_sum', 'V40_divide_V40_YEAR_min', 'FE_divide_FE_FE_woe_bin_max', 'PQINDEX_divide_PQINDEX_YEAR_COMPONENT_median', 'ANONYMOUS_1_V40_minus_ANONYMOUS_1_V40_YEAR_COMPONENT_sum', 'ANONYMOUS_2_divide_ANONYMOUS_2_FE_woe_bin_max', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_COMPONENT_sum', 'V40_divide_V40_FE_woe_bin_sum', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_YEAR_COMPONENT_median', 'ANONYMOUS_1_FE_YEAR_median', 'ZN_divide_ZN_YEAR_min', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_min', 'ZN_minus_ZN_FE_woe_bin_sum', 'PQINDEX_divide_PQINDEX_YEAR_min', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_FE_woe_bin_min', 'CU_divide_CU_YEAR_COMPONENT_min', 'ANONYMOUS_2_YEAR_max', 'CU_minus_CU_FE_woe_bin_sum', 'ANONYMOUS_1_FE_minus_ANONYMOUS_1_FE_YEAR_max', 'FE_minus_FE_FE_woe_bin_sum', 'FE_divide_FE_YEAR_max', 'PQINDEX_minus_PQINDEX_FE_woe_bin_max', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_sum', 'FE_minus_FE_YEAR_max', 'ANONYMOUS_1_minus_ANONYMOUS_1_YEAR_COMPONENT_min', 'FE_minus_FE_FE_woe_bin_min', 'cluster_no_by_ANONYMOUS_1_FE_median', 'ANONYMOUS_1_CR_minus_ANONYMOUS_1_CR_YEAR_COMPONENT_median', 'ZN_minus_ZN_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_CR_divide_ANONYMOUS_1_CR_FE_woe_bin_sum', 'ZN_minus_ZN_YEAR_sum', 'NI_V40', 'ANONYMOUS_2_minus_ANONYMOUS_2_YEAR_max', 'CU_minus_CU_YEAR_sum', 'V40_minus_V40_FE_woe_bin_min', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_FE_woe_bin_median', 'PQINDEX_divide_PQINDEX_FE_woe_bin_max', 'ZN_minus_ZN_FE_woe_bin_max', 'V40_minus_V40_COMPONENT_ARBITRARY_min', 'FE_minus_FE_YEAR_COMPONENT_median', 'ANONYMOUS_1_FE', 'PQINDEX_minus_PQINDEX_FE_woe_bin_median', 'PQINDEX_minus_PQINDEX_YEAR_max', 'FE_YEAR_COMPONENT_median', 'V40_divide_V40_COMPONENT_ARBITRARY_min', 'ANONYMOUS_1_V40_divide_ANONYMOUS_1_V40_FE_woe_bin_sum', 'ZN_minus_ZN_COMPONENT_ARBITRARY_max']\ndrop_features_vc4 = ['PQINDEX_divide_PQINDEX_COMPONENT_ARBITRARY_median', 'ZN_minus_ZN_YEAR_COMPONENT_min', 'ANONYMOUS_1_ZN_minus_ANONYMOUS_1_ZN_YEAR_sum', 'ANONYMOUS_1_ZN_divide_ANONYMOUS_1_ZN_YEAR_median', 'PQINDEX_divide_PQINDEX_YEAR_median', 'ANONYMOUS_2_minus_ANONYMOUS_2_YEAR_COMPONENT_max', 'FE_divide_FE_FE_woe_bin_min', 'V40_divide_V40_COMPONENT_ARBITRARY_median', 'ANONYMOUS_1_divide_ANONYMOUS_1_YEAR_COMPONENT_max']\n\nparams =  {\n    'learning_rate': 0.07398,\n    'max_depth': 4.309,\n    'colsample_bytree': 0.4028,\n    'subsample': 0.4278,\n    'min_child_samples': 25.65,\n    'min_child_weight': 0.6138,\n    'min_split_gain': 0.7354,\n    'num_leaves': 62.68,\n    'reg_alpha': 0.2889,\n    'reg_lambda': 7.875\n}\n\n\nseed_num = [113,176]\n\ntrain_model(train,test,params,True,5,drop_features_vc1,seed_num=seed_num)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:39:32.850496Z","iopub.execute_input":"2022-11-27T06:39:32.851213Z","iopub.status.idle":"2022-11-27T06:40:24.714990Z","shell.execute_reply.started":"2022-11-27T06:39:32.851164Z","shell.execute_reply":"2022-11-27T06:40:24.713765Z"},"trusted":true},"execution_count":330,"outputs":[{"name":"stdout","text":"--------------------------------------------------\n>> seed_num: [113, 176]\n>> drop_features: 195\n113\nFold  1 AUC : 0.753391\nFold  2 AUC : 0.705738\nFold  3 AUC : 0.740672\nFold  4 AUC : 0.722189\nFold  5 AUC : 0.685271\n176\nFold  1 AUC : 0.754289\nFold  2 AUC : 0.712755\nFold  3 AUC : 0.734301\nFold  4 AUC : 0.721653\nFold  5 AUC : 0.690955\nFull AUC score 0.711661\nthred: 0.1016\nncol 428\nauc: 0.7116613591625179\nf1: 0.5453435931922401\nTarget ratio(real): 0.04085393948702196\nTarget ratio(pred): 0.030458533355404733\nTarget sum: 184\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\n--------------------------------------------------\n>> seed_num: [113, 176]\n>> drop_features: 195\n113\nFold  1 AUC : 0.696885\nFold  2 AUC : 0.717332\nFold  3 AUC : 0.706659\nFold  4 AUC : 0.716143\nFold  5 AUC : 0.687260\n176\nFold  1 AUC : 0.699257\nFold  2 AUC : 0.718767\nFold  3 AUC : 0.699679\nFold  4 AUC : 0.716366\nFold  5 AUC : 0.691165\nFull AUC score 0.705168\nthred: 0.1627\nncol 428\nauc: 0.7051682189190381\nf1: 0.5904704437462381\nTarget ratio(real): 0.08534941468605889\nTarget ratio(pred): 0.09005131600728357\nTarget sum: 544\nadd Code\n--------------------------------------------------\n>> seed_num: 176\n>> drop_features: 195\nFold  1 AUC : 0.699257\nFold  2 AUC : 0.718767\nFold  3 AUC : 0.699679\nFold  4 AUC : 0.716366\nFold  5 AUC : 0.691165\nFull AUC score 0.704596\nthred: 0.1636\nncol 430\nauc: 0.7045962957432151\nf1: 0.5922652586883465\nTarget ratio(real): 0.08534941468605889\nTarget ratio(pred): 0.08872703194835292\nTarget sum: 536\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:40:24.716809Z","iopub.execute_input":"2022-11-27T06:40:24.717159Z","iopub.status.idle":"2022-11-27T06:40:24.725517Z","shell.execute_reply.started":"2022-11-27T06:40:24.717126Z","shell.execute_reply":"2022-11-27T06:40:24.724330Z"},"trusted":true},"execution_count":331,"outputs":[{"execution_count":331,"output_type":"execute_result","data":{"text/plain":"'\\n--------------------------------------------------\\n>> seed_num: [113, 176]\\n>> drop_features: 195\\n113\\nFold  1 AUC : 0.696885\\nFold  2 AUC : 0.717332\\nFold  3 AUC : 0.706659\\nFold  4 AUC : 0.716143\\nFold  5 AUC : 0.687260\\n176\\nFold  1 AUC : 0.699257\\nFold  2 AUC : 0.718767\\nFold  3 AUC : 0.699679\\nFold  4 AUC : 0.716366\\nFold  5 AUC : 0.691165\\nFull AUC score 0.705168\\nthred: 0.1627\\nncol 428\\nauc: 0.7051682189190381\\nf1: 0.5904704437462381\\nTarget ratio(real): 0.08534941468605889\\nTarget ratio(pred): 0.09005131600728357\\nTarget sum: 544\\nadd Code\\n--------------------------------------------------\\n>> seed_num: 176\\n>> drop_features: 195\\nFold  1 AUC : 0.699257\\nFold  2 AUC : 0.718767\\nFold  3 AUC : 0.699679\\nFold  4 AUC : 0.716366\\nFold  5 AUC : 0.691165\\nFull AUC score 0.704596\\nthred: 0.1636\\nncol 430\\nauc: 0.7045962957432151\\nf1: 0.5922652586883465\\nTarget ratio(real): 0.08534941468605889\\nTarget ratio(pred): 0.08872703194835292\\nTarget sum: 536\\n'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}